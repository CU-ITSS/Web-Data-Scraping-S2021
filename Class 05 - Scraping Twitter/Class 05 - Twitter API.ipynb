{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2021 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) â€” ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping web data with Selenium\n",
    "* **Week 4**: Scraping an API with `requests` and `json`, Wikipedia and Reddit\n",
    "* **Week 5**: Scraping data from Twitter\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "Thank you also to Professor [Terra KcKinnish](https://www.colorado.edu/economics/people/faculty/terra-mckinnish) for coordinating the ITSS seminars.\n",
    "\n",
    "## Class 5 goals\n",
    "\n",
    "* Sharing accomplishments and challenges with last week's material\n",
    "* Using the `twitter` wrapper library to handle authentication\n",
    "* Retrieving and parsing a single tweet\n",
    "* Rehydrating a list of tweet IDs\n",
    "* Pulling a user's timeline\n",
    "* Pulling a user's friend and follower lists\n",
    "* Using the search endpoint of the API\n",
    "* Listen to the streaming API\n",
    "* Detecting bot accounts using IU's Bot-o-Meter\n",
    "\n",
    "Start with our usual suspect packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Jupyter Notebook display images in-line\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# Import our helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import quote, unquote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a library called VADER to help with sentiment analysis of tweets. We need to do some setup first! You should only need to do this step once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the VADER sentiment analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate the model\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing a Twitter API wrapper\n",
    "\n",
    "As was the case with Reddit, we will take advantage of a wrapper library to handle the heavy lifting of authenticating, making specific requests, handling rate-limiting, *etc*. There are no shortage of Python wrappers for the Twitter API, but the most popular are:\n",
    "\n",
    "* [twitter](https://github.com/python-twitter-tools/twitter)\n",
    "* [python-twitter](https://python-twitter.readthedocs.io/en/latest/)\n",
    "* [Tweepy](http://docs.tweepy.org/en/latest/)\n",
    "* [Twython](https://twython.readthedocs.io/en/latest/)\n",
    "\n",
    "There are other wrapper libraries linked from the [Twitter developer utilities documentation](https://developer.twitter.com/en/docs/twitter-api/tools-and-libraries).\n",
    "\n",
    "I'm going to use `twitter` just because it is very lightweight and replicates the official Twitter API's design.\n",
    "\n",
    "You will need to install this since it does not come with conda by default. At the Terminal:\n",
    "\n",
    "`pip install twitter`\n",
    "\n",
    "Once you've installed it, you can import the `twitter` wrapper library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticating\n",
    "I don't want to share my Twitter credentials with the world, so I load from from my local machine. If you wanted to do this, it should take this format of:\n",
    "\n",
    "```\n",
    "{\"consumer_key\":\"API key\",\n",
    " \"consumer_secret\":\"API secret key\",\n",
    " \"access_token_key\":\"Access token\",\n",
    " \"access_token_secret\":\"Access token secret\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load my key information from disk\n",
    "with open('twitter_keys.json','r') as f:\n",
    "    twitter_keys = json.load(f)\n",
    "\n",
    "# Authenticate with the Twitter API using the twitter_keys dictionary\n",
    "# The \"tweet_mode='extended' allows us to see the full 280 characters in tweets\n",
    "api = twitter.Twitter(auth=twitter.OAuth(twitter_keys['access_token_key'],\n",
    "                                         twitter_keys['access_token_secret'],\n",
    "                                         twitter_keys['consumer_key'],\n",
    "                                         twitter_keys['consumer_secret']),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can just enter your keys directly into the `Api` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key = 'API key',\n",
    "                  consumer_secret = 'API secret key',\n",
    "                  access_token_key = 'Access token',\n",
    "                  access_token_secret = 'Access token secret',\n",
    "                  tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that you can connect to the API. Retrieve *Daily Camera* journalist [@mitchellbyars](https://twitter.com/mitchellbyars)'s account information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.users.show(screen_name='mitchellbyars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve his most recent tweets. Obnoxiously, you also need to add a parameter `tweet_mode='extended'` ([docs](https://developer.twitter.com/en/docs/twitter-api/tweets/timelines/migrate/standard-to-twitter-api-v2)) to get the full 280 characters of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.statuses.user_timeline(screen_name='mitchellbyars',\n",
    "                           count=5,\n",
    "                           tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the payload of a single tweet\n",
    "\n",
    "Wikipedia helpfully maintains a [List of most-retweeted tweets](https://en.wikipedia.org/wiki/List_of_most-retweeted_tweets). Go to one of the tweets and pull out the ID at the end of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = api.statuses.show(_id='849813577770778624')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the attributes of this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When was the tweet created\n",
    "tweet['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of favorites (at the time of the API call)\n",
    "tweet['favorite_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of retweets (at the time of the API call)\n",
    "tweet['retweet_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text of the tweet\n",
    "tweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location (if it geo-located)\n",
    "tweet['geo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hashtags present\n",
    "tweet['entities']['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet ID\n",
    "tweet['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A guess at the language of the tweet\n",
    "tweet['lang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next two attributes return `User` and `Media` objects rather than simple strings, ints, *etc*. that have their own attributes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access attributes of this `User` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen name of the user\n",
    "tweet['user']['screen_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displayed name of the user\n",
    "tweet['user']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User biography\n",
    "tweet['user']['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account creation time\n",
    "tweet['user']['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-reported location\n",
    "tweet['user']['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets from the user\n",
    "tweet['user']['statuses_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of followers\n",
    "tweet['user']['followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of friends (accounts this account follows, followees, etc.)\n",
    "tweet['user']['friends_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `Media` object inside this list contains information about the type and the URLs of the media inside this object. If there were multiple images in this tweet, there would be a `Media` item in the list for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['entities']['media'][0]['media_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rehydrating a list of tweets\n",
    "\n",
    "Twitter's Terms of Service do not allow datasets of statuses to be shared, but researchers are permitted to share the identifiers for tweets in their datasets. Researchers then need to \"rehydrate\" these statuses by requesting the full payloads from Twitter's API. A list of resources with links to tweet IDs used in research:\n",
    "\n",
    "* [DocNow's Tweet ID Datasets](https://www.docnow.io/catalog/)\n",
    "* [FollowTheHashtag's Free Twitter Datasets](http://followthehashtag.com/datasets/)\n",
    "* [AcademicTorrents](http://academictorrents.com/browse.php?search=twitter)\n",
    "* [FiveThirtyEight's Russian Troll Tweets](https://github.com/fivethirtyeight/russian-troll-tweets/)\n",
    "* [Harvard Dataverse](https://dataverse.harvard.edu/dataverse/harvard?q=twitter&types=datasets&sort=score&order=desc&page=1)\n",
    "\n",
    "This has some privacy benefits: Twitter's [compliance statement](https://developer.twitter.com/en/docs/twitter-api/enterprise/compliance-firehose-api/overview) describes that users should retain the option to delete tweets or their accounts and this rehydration arrangementâ€”theoreticallyâ€”prevents their tweet content from circulating without their consent. In practice, many of the largest Twitter corpora come from the streaming API (more on that later in this notebook) and Twitter has a \"[compliance stream](https://developer.twitter.com/en/docs/tweets/compliance/api-reference/compliance-firehose)\" that indicates that a user has deleted a tweet, protected their account, Twitter has suspended an account, Twitter has withheld the status, *etc*. and the tweet should be removed from your streaming dataset as well. The Sunlight Foundation and ProPublica maintain a list of deleted tweets from politicians called [Politiwoops](https://projects.propublica.org/politwoops/).\n",
    "\n",
    "I am going to use a [list of tweets made by Senators in the 115th Congress](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/UIVHQR) collected by Justin Littman in 2017. Load the text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('senators-115.txt','r') as f:\n",
    "    senators_tweet_ids = [tweet_id.strip() for tweet_id in f.readlines()]\n",
    "    \n",
    "\"There are {0:,} tweets IDs in the file.\".format(len(senators_tweet_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senators_tweet_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `statuses.lookup` API endpoint, which accepts a string containing up to 100 comma-separated tweet IDs. We will use the \"`map=True`\" parameter to keep track of any tweets that were not returned (which should be `None`s rather than `Status`es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senators_10_tweets = api.statuses.lookup(_id=','.join(senators_tweet_ids[:50]),\n",
    "                                         map=True,\n",
    "                                         tweet_mode='extended'\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(senators_10_tweets['id'].values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a bit of accounting on rate limits. According to the API documentation for [get statuses/lookup](https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-statuses-lookup), you can ask for up to 100 tweets per request and you can make 900 requests per 15-minutes. This means you can theoretically rehydrate 90,000 tweets per 15 minutes, or 360,000 tweets per hour. So it would take approximately 90 minutes to rehydrate all 500,000 of these senators' tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the field with the rate limit status for looking up statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.application.rate_limit_status()['resources']['statuses']['/statuses/lookup']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse out when the the API limit will reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the reset field\n",
    "reset_datetime = api.application.rate_limit_status()['resources']['statuses']['/statuses/lookup']['reset']\n",
    "\n",
    "# Convert from UNIX time to something interpretable\n",
    "print(datetime.fromtimestamp(reset_datetime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 10 `Status` objects returned by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senators_10_tweets['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop to go through the ten tweets and print out information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in senators_10_tweets['id'].values():\n",
    "    screen_name = status['user']['screen_name']\n",
    "    created = pd.to_datetime(status['created_at'])\n",
    "    text = status['full_text']\n",
    "    retweets = status['retweet_count']\n",
    "    formatted_str = '{0} on {1} said {2}, which received {3} retweets.\\n'\n",
    "    print(formatted_str.format(screen_name,created,text,retweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, extract the relevant fields, save these as a list of dictionaries, and convert the list of dictionaries to a DataFrame. Note that when an account retweets another account, a second `Status` object is embedded under the \"`.retweeted_status`\" attribute that contains the parent tweet's information. In these cases, the \"`.created_at`\" from the Senator's account is when s/he retweeted the status and the \"`.created_at`\" for the \"`.retweeted_status`\" is when the parent tweet was first posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(senators_10_tweets['id'].values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store the data after we process it below\n",
    "statuses_list = []\n",
    "\n",
    "# Loop through each of the status objects\n",
    "for status in senators_10_tweets['id'].values():\n",
    "    \n",
    "    # Check to make sure the status is not empty/None \n",
    "    if status != None:\n",
    "        \n",
    "        # Create an empty dictionary to store relevant fields\n",
    "        payload = {}\n",
    "        payload['id'] = status['id_str']\n",
    "        payload['screen_name'] = status['user']['screen_name']\n",
    "        payload['created'] = pd.to_datetime(status['created_at'])\n",
    "        payload['retweets'] = status['retweet_count']\n",
    "        payload['favorites'] = status['favorite_count']\n",
    "\n",
    "        # If an account retweets another account, we should store that information\n",
    "        if 'retweeted_status' in status:\n",
    "            payload['text'] = status['retweeted_status']['full_text']\n",
    "            payload['retweeted'] = True\n",
    "            payload['retweeted_screen_name'] = status['retweeted_status']['user']['screen_name']\n",
    "            payload['retweeted_created'] = status['retweeted_status']['created_at']\n",
    "            \n",
    "        # If there is no retweeted_status then it's not a retweet\n",
    "        else:\n",
    "            payload['text'] = status['full_text']\n",
    "            payload['retweeted'] = False\n",
    "            payload['retweeted_screen_name'] = False\n",
    "            payload['retweeted_created'] = False\n",
    "\n",
    "        # Store the payload dictionary in our list\n",
    "        statuses_list.append(payload)\n",
    "        \n",
    "# Conver to a DataFrame\n",
    "df = pd.DataFrame(statuses_list)\n",
    "\n",
    "# Inspect\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling a user's timeline\n",
    "\n",
    "In general, if you want to retrieve the tweets from a user's timeline, we can use Twitter's API to get the 3,200 most recent tweets from the [get statuses/user_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html) API endpoint. This will include retweets of other statuses. We can retrieve up to 200 tweets per request and can make 900 requests per 15-minute window, so we can get 18,000 tweets per window or 72,000 tweets per hour. This means we could theoretically get up to 22 users' most recent 3,200 tweets per hour.\n",
    "\n",
    "Alexandria Ocasio-Cortez has written 6,975 tweets on her personal account, \"[AOC](https://twitter.com/aoc)\". She also has an official account \"[RepAOC](https://twitter.com/repaoc)\", but this only has 14 tweets. Let's get her 3,200 most-recent tweets from the API. Disappointingly, `python-twitter` does not handle the \"pagination\" for us so we can only ask for 200 tweets at a time and have to update when to ask for the next tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets = api.statuses.user_timeline(screen_name='aoc',\n",
    "                                        count=200,\n",
    "                                        tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:,} tweets were returned.\".format(len(aoc_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tweet returned is the most recent tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets[0]['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last tweet returned (the 200th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets[-1]['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really care about the final tweet's ID so we can make an API query that asks for the next 200 statuses before the last tweet returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets[-1]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the next query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets2 = api.statuses.user_timeline(screen_name='aoc',\n",
    "                                         count=200,\n",
    "                                         tweet_mode='extended',\n",
    "                                         include_rts=True,\n",
    "                                         max_id=aoc_tweets[-1]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the first tweet in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets2[0]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the last tweet in the first set (`aoc_tweets`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoc_tweets[-1]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check in on how much of our API rate limit we've used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.application.rate_limit_status()['resources']['statuses']['/statuses/user_timeline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write a loop to get all 3,200 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the list of the 200 most-recent tweets\n",
    "aoc_timeline_tweets = api.statuses.user_timeline(screen_name='aoc',\n",
    "                                         count=200,\n",
    "                                         tweet_mode='extended',\n",
    "                                         include_rts=True)\n",
    "\n",
    "# Initialize a counter so we don't go overboard with our requests\n",
    "request_counter = 1\n",
    "\n",
    "# While our request counter hasn't gone past 16, but try for more :)\n",
    "while request_counter < 20:\n",
    "    # Get the most oldest tweet id\n",
    "    final_status_id = aoc_timeline_tweets[-1]['id']\n",
    "    \n",
    "    # Pass this tweet ID into the max_id parameter, minus 1 so we don't duplicate it\n",
    "    aoc_timeline_tweets += api.statuses.user_timeline(screen_name='aoc',\n",
    "                                                  count=200,\n",
    "                                                  tweet_mode='extended',\n",
    "                                                  include_rts=True,\n",
    "                                                  max_id=final_status_id-1)\n",
    "    \n",
    "    # Increment our request_counter\n",
    "    request_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just used a chunk of my API rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.application.rate_limit_status()['resources']['statuses']['/statuses/user_timeline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow a few more tweets snuck in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aoc_timeline_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also abstract this into a function we could use for anyone's timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_user_timeline(screen_name,count=200,include_rts=True,exclude_replies=False):\n",
    "    \n",
    "    # Start with the list of the 200 most-recent tweets\n",
    "    timeline_tweets = api.statuses.user_timeline(screen_name = screen_name,\n",
    "                                                 count = count,\n",
    "                                                 tweet_mode = 'extended',\n",
    "                                                 include_rts = include_rts,\n",
    "                                                 exclude_replies = exclude_replies\n",
    "                                                )\n",
    "\n",
    "    # Initialize a counter so we don't go overboard with our requests\n",
    "    request_counter = 1\n",
    "\n",
    "    # While our request counter hasn't gone past 16, but try for 20\n",
    "    while request_counter < 20:\n",
    "        # Get the most oldest tweet id\n",
    "        final_status_id = timeline_tweets[-1]['id']\n",
    "\n",
    "        # Pass this tweet ID into the max_id parameter, minus 1 so we don't duplicate it\n",
    "        timeline_tweets += api.statuses.user_timeline(screen_name = screen_name,\n",
    "                                                      count = count,\n",
    "                                                      tweet_mode = 'extended',\n",
    "                                                      include_rts = include_rts,\n",
    "                                                      exclude_replies = exclude_replies,\n",
    "                                                      max_id = final_status_id-1)\n",
    "\n",
    "        # Increment our request_counter\n",
    "        request_counter += 1\n",
    "        \n",
    "    return timeline_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this on [@joebiden](https://twitter.com/joebiden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_tweets = get_all_user_timeline('joebiden')\n",
    "len(biden_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've added a lot more sugar into our loop to grab information about replies, user mentions, and hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store the data after we process it below\n",
    "statuses_list = []\n",
    "\n",
    "# Loop through each of the status objects\n",
    "for status in biden_tweets:\n",
    "    \n",
    "    # Check to make sure the status is not empty/None \n",
    "    if status != None:\n",
    "        \n",
    "        # Create an empty dictionary to store relevant fields\n",
    "        payload = {}\n",
    "        payload['id'] = status['id_str']\n",
    "        payload['screen_name'] = status['user']['screen_name']\n",
    "        payload['created'] = pd.to_datetime(status['created_at'])\n",
    "        payload['retweets'] = status['retweet_count']\n",
    "        payload['favorites'] = status['favorite_count']\n",
    "        payload['reply_screen_name'] = status['in_reply_to_screen_name']\n",
    "        payload['reply_id'] = status['in_reply_to_status_id']\n",
    "        payload['source'] = BeautifulSoup(status['source']).text\n",
    "\n",
    "        if len(status['entities']['user_mentions']) > 0:\n",
    "            payload['user_mentions'] = '; '.join([m['screen_name'] for m in status['entities']['user_mentions']])\n",
    "        else:\n",
    "            payload['user_mentions'] = None\n",
    "            \n",
    "        if len(status['entities']['hashtags']) > 0:\n",
    "            payload['hashtags'] = '; '.join([h['text'] for h in status['entities']['hashtags']])\n",
    "        else:\n",
    "            payload['hashtags'] = None\n",
    "        \n",
    "        # If an account retweets another account, we should store that information\n",
    "        if 'retweeted_status' in status:\n",
    "            payload['text'] = status['retweeted_status']['full_text']\n",
    "            payload['retweeted'] = True\n",
    "            payload['retweeted_screen_name'] = status['retweeted_status']['user']['screen_name']\n",
    "            payload['retweeted_created'] = status['retweeted_status']['created_at']\n",
    "            payload['retweeted_source'] = BeautifulSoup(status['retweeted_status']['source']).text\n",
    "            if len(status['retweeted_status']['entities']['hashtags']) > 0:\n",
    "                payload['hashtags'] = '; '.join([h['text'] for h in status['retweeted_status']['entities']['hashtags']])\n",
    "            else:\n",
    "                payload['hashtags'] = None\n",
    "        # If there is no retweeted_status then it's not a retweet\n",
    "        else:\n",
    "            payload['text'] = status['full_text']\n",
    "            payload['retweeted'] = False\n",
    "            payload['retweeted_screen_name'] = False\n",
    "            payload['retweeted_created'] = False\n",
    "\n",
    "        # Store the payload dictionary in our list\n",
    "        statuses_list.append(payload)\n",
    "        \n",
    "# Conver to a DataFrame\n",
    "df = pd.DataFrame(statuses_list)\n",
    "\n",
    "# Inspect\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the \"created\" column into a proper `datetime` object and extract the dates as another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['created'])\n",
    "df['date'] = df['timestamp'].apply(lambda x:x.date())\n",
    "df['weekday'] = df['timestamp'].apply(lambda x:x.weekday)\n",
    "df['hour'] = df['timestamp'].apply(lambda x:x.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the number of tweets by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date and aggregate by number of tweets on that date\n",
    "_s = df.groupby(pd.Grouper(key='timestamp',freq='1D')).agg({'id':len,'retweeted':'sum','reply_id':lambda x:sum(x.notnull())})\n",
    "\n",
    "# Reindex the data to be continuous over the range, fill in missing dates as 0s\n",
    "_s.columns = ['Tweets','Retweets','Replies']\n",
    "_s_frac = _s[['Retweets','Replies']].div(_s['Tweets'],axis=0).fillna(0)\n",
    "\n",
    "# Make the plot\n",
    "f,axs = plt.subplots(2,1,figsize=(8,6),sharex=True)\n",
    "_s['Tweets'].rolling(3).mean().plot(ax=axs[0])\n",
    "_s_frac.rolling(3).mean().plot(ax=axs[1],legend=False)\n",
    "\n",
    "axs[0].legend(loc='center left',bbox_to_anchor=(1,.5))\n",
    "axs[1].legend(loc='center left',bbox_to_anchor=(1,.5))\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[1].set_ylabel('Fraction of tweets')\n",
    "\n",
    "# Annotate the plot with lines corresponding to major events\n",
    "for ax in axs:\n",
    "    ax.axvline(pd.Timestamp('2020-04-08'),lw=3,c='k',alpha=.25) # Biden cinches\n",
    "    ax.axvline(pd.Timestamp('2020-08-20'),lw=3,c='k',alpha=.25) # DNC speech\n",
    "    ax.axvline(pd.Timestamp('2020-11-03'),lw=3,c='k',alpha=.25) # Election day\n",
    "    ax.axvline(pd.Timestamp('2021-01-21'),lw=3,c='k',alpha=.25) # Swearing in\n",
    "    \n",
    "axs[0].text(x=pd.Timestamp('2020-04-08')+pd.Timedelta(3,'d'),y=47.5,s='Biden\\ncinches',va='center')\n",
    "axs[0].text(x=pd.Timestamp('2020-08-20')+pd.Timedelta(3,'d'),y=47.5,s='DNC\\nspeech',va='center')\n",
    "axs[0].text(x=pd.Timestamp('2020-11-03')+pd.Timedelta(3,'d'),y=47.5,s='Election',va='center')\n",
    "axs[0].text(x=pd.Timestamp('2021-01-21')+pd.Timedelta(3,'d'),y=47.5,s='Sworn\\nin',va='center')\n",
    "\n",
    "f.tight_layout()\n",
    "# f.savefig('aoc_activity.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the date and aggregate by the sum of retweets and favorites for all tweets on that date\n",
    "_s = df.groupby(pd.Grouper(key='timestamp',freq='1D')).agg({'retweets':'sum','favorites':'sum'})\n",
    "\n",
    "# Make the plot\n",
    "f,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "ax = _s.rolling(3).mean().plot(legend=False,lw=2,ax=ax)\n",
    "ax.set_ylim((0,7000000))\n",
    "ax.legend(loc='center left',bbox_to_anchor=(1,.5))\n",
    "ax.set_title('Daily engagement with @joebiden tweets')\n",
    "\n",
    "# Annotate the plot with lines corresponding to major events\n",
    "ax.axvline(pd.Timestamp('2020-04-08'),lw=3,c='k',alpha=.25) # Biden cinches\n",
    "ax.axvline(pd.Timestamp('2020-08-20'),lw=3,c='k',alpha=.25) # DNC speech\n",
    "ax.axvline(pd.Timestamp('2020-11-03'),lw=3,c='k',alpha=.25) # Election day\n",
    "ax.axvline(pd.Timestamp('2021-01-21'),lw=3,c='k',alpha=.25) # Swearing in\n",
    "    \n",
    "_y = 6.5e6\n",
    "ax.text(x=pd.Timestamp('2020-04-08')+pd.Timedelta(3,'d'),y=_y,s='Biden\\ncinches',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-08-20')+pd.Timedelta(3,'d'),y=_y,s='DNC\\nspeech',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-11-03')+pd.Timedelta(3,'d'),y=_y,s='Election',va='center')\n",
    "ax.text(x=pd.Timestamp('2021-01-21')+pd.Timedelta(3,'d'),y=_y,s='Sworn\\nin',va='center')\n",
    "\n",
    "f.tight_layout()\n",
    "# f.savefig('joebiden_engagement.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a bit of sentiment analysis. You'll likely need to [install the NLTK data](https://www.nltk.org/data.html) for this to work. We are going to use the [VADER sentiment analysis tool](https://github.com/cjhutto/vaderSentiment) that was specifically trained for social media text: [see paper here](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment scores for each tweet's text\n",
    "df['sentiment'] = df['text'].apply(lambda x:sia.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot out the daily sentiment of tweets with major events annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the date and aggregate by the average sentiment for all tweets on that date\n",
    "_s = df.groupby(pd.Grouper(key='timestamp',freq='1D')).agg({'sentiment':'mean'})\n",
    "\n",
    "# Make the plot with a 7-day rolling average\n",
    "f,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "ax = _s.rolling(3).mean().fillna(method='ffill').plot(legend=False,ax=ax)\n",
    "ax.set_ylim((-.3,.8))\n",
    "ax.axhline(0,ls='--',c='k',lw=1)\n",
    "ax.set_title('Sentiment of @joebiden tweets')\n",
    "ax.set_ylabel('Compound VADER score')\n",
    "\n",
    "# Annotate the plot with lines corresponding to major events\n",
    "ax.axvline(pd.Timestamp('2020-04-08'),lw=3,c='k',alpha=.25) # Biden cinches\n",
    "ax.axvline(pd.Timestamp('2020-08-20'),lw=3,c='k',alpha=.25) # DNC speech\n",
    "ax.axvline(pd.Timestamp('2020-11-03'),lw=3,c='k',alpha=.25) # Election day\n",
    "ax.axvline(pd.Timestamp('2021-01-21'),lw=3,c='k',alpha=.25) # Swearing in\n",
    "    \n",
    "_y = .7\n",
    "ax.text(x=pd.Timestamp('2020-04-08')+pd.Timedelta(3,'d'),y=_y,s='Biden\\ncinches',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-08-20')+pd.Timedelta(3,'d'),y=_y,s='DNC\\nspeech',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-11-03')+pd.Timedelta(3,'d'),y=_y,s='Election',va='center')\n",
    "ax.text(x=pd.Timestamp('2021-01-21')+pd.Timedelta(3,'d'),y=_y,s='Sworn\\nin',va='center')\n",
    "\n",
    "f.tight_layout()\n",
    "# f.savefig('joebiden_sentiment.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the engagement for @joebiden tweets, ignoring retweets and replies, and normalizing for total tweet activity on that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = ~df['retweeted']\n",
    "c2 = df['reply_id'].isnull()\n",
    "\n",
    "pure_tweets_df = df[c1 & c2]\n",
    "print(\"There are {0:,} tweets that are not retweets or replies.\".format(len(pure_tweets_df)))\n",
    "\n",
    "_s = pure_tweets_df.groupby(pd.Grouper(key='timestamp',freq='1D')).agg({'retweets':'sum','favorites':'sum','id':len})\n",
    "_s = _s[['retweets','favorites']].div(_s['id'],axis=0)\n",
    "_s.columns = ['Retweets','Favorites']\n",
    "\n",
    "# Make the plot\n",
    "f,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "ax = _s.rolling(3).mean().plot(legend=False,lw=2,ax=ax)\n",
    "# ax.set_yscale('symlog')\n",
    "# ax.set_ylim((1e1,1e6))\n",
    "\n",
    "ax.legend(loc='center left',bbox_to_anchor=(1,.5))\n",
    "ax.set_title('Daily engagement with @joebiden tweets')\n",
    "ax.set_ylabel('Engagement per tweet')\n",
    "\n",
    "# Annotate the plot with lines corresponding to major events\n",
    "ax.axvline(pd.Timestamp('2020-04-08'),lw=3,c='k',alpha=.25) # Biden cinches\n",
    "ax.axvline(pd.Timestamp('2020-08-20'),lw=3,c='k',alpha=.25) # DNC speech\n",
    "ax.axvline(pd.Timestamp('2020-11-03'),lw=3,c='k',alpha=.25) # Election day\n",
    "ax.axvline(pd.Timestamp('2021-01-21'),lw=3,c='k',alpha=.25) # Swearing in\n",
    "    \n",
    "_y = 5.5e5\n",
    "ax.text(x=pd.Timestamp('2020-04-08')+pd.Timedelta(3,'d'),y=_y,s='Biden\\ncinches',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-08-20')+pd.Timedelta(3,'d'),y=_y,s='DNC\\nspeech',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-11-03')+pd.Timedelta(3,'d'),y=_y,s='Election',va='center')\n",
    "ax.text(x=pd.Timestamp('2021-01-21')+pd.Timedelta(3,'d'),y=_y,s='Sworn\\nin',va='center')\n",
    "\n",
    "f.tight_layout()\n",
    "# f.savefig('aoc_engagement_no_rt_reply.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot favorites per retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "\n",
    "(_s['Retweets']/_s['Favorites']).fillna(0).rolling(3).mean().plot(ax=ax)\n",
    "\n",
    "ax.set_ylim((0,.5))\n",
    "\n",
    "# Annotate the plot with lines corresponding to major events\n",
    "ax.axvline(pd.Timestamp('2020-04-08'),lw=3,c='k',alpha=.25) # Biden cinches\n",
    "ax.axvline(pd.Timestamp('2020-08-20'),lw=3,c='k',alpha=.25) # DNC speech\n",
    "ax.axvline(pd.Timestamp('2020-11-03'),lw=3,c='k',alpha=.25) # Election day\n",
    "ax.axvline(pd.Timestamp('2021-01-21'),lw=3,c='k',alpha=.25) # Swearing in\n",
    "    \n",
    "_y = .45\n",
    "ax.text(x=pd.Timestamp('2020-04-08')+pd.Timedelta(3,'d'),y=_y,s='Biden\\ncinches',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-08-20')+pd.Timedelta(3,'d'),y=_y,s='DNC\\nspeech',va='center')\n",
    "ax.text(x=pd.Timestamp('2020-11-03')+pd.Timedelta(3,'d'),y=_y,s='Election',va='center')\n",
    "ax.text(x=pd.Timestamp('2021-01-21')+pd.Timedelta(3,'d'),y=_y,s='Sworn\\nin',va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top tweets by retweets per favorite? The're primarily from before her primary win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rt_fav_ratio'] = (df['retweets']/df['favorites']).replace({np.inf:np.nan})\n",
    "top_retweets = df['rt_fav_ratio'].dropna().sort_values(ascending=False).head(10)\n",
    "df.loc[top_retweets.index,['created','text','retweets','favorites']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an intereseting relationship between seniment and retweet/favorite ratio? We can specify a simple univariate LOESS regression for the relationship between sentiment and the retweet-per-favorite ratio. It appears that extremely negative and positive tweets have higher ratios than neutral tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.lmplot(x='sentiment',y='rt_fav_ratio',data=df,lowess=True,aspect=2,\n",
    "              line_kws={'color':'red','linewidth':10,'alpha':.5})\n",
    "ax = g.axes[0,0]\n",
    "ax.set_ylim((0,.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling a user's friends\n",
    "\n",
    "In the parlance of the Twitter API, the people who follow an account are \"followers\" and the people followed by an account are \"friends\". There's unfortuantely no timestamp meta-data about when friend and follower relationships were created. The API limits on this are much more stringent than other API calls: only 200 accounts per request and only 15 requests per 15-minute window: basically 200 accounts per minute or 3,000 accounts before you hit the rate limit. AOC has 1,417 friends, so it takes 8 API requests to get them all, leaving me with 7 requests in this 15-minute window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = api.friends.list(screen_name='joebiden',count=200,skip_status=True)\n",
    "print(\"There are {0:,} friends.\".format(len(friends['users'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check my API rate limit status too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.application.rate_limit_status()['resources']['friends']['/friends/list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.fromtimestamp(api.application.rate_limit_status()['resources']['friends']['/friends/list']['reset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think \"friends\" convey much more valuable information about an account than followers, primarily because an account doesn't choose who follows them. However, if you wanted to get the followers of an account, we use the `GetFollowers` method. I'm only going to grab 200 so I don't burn more API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers = api.followers.list(screen_name='joebiden',skip_status=True,total_count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.application.rate_limit_status()['resources']['followers']['/followers/list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access these user objects to pull out interesting meta-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['screen_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['statuses_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['friends_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends['users'][0]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the friends of @joebiden and turn it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_payloads = []\n",
    "\n",
    "for friend in friends['users']:\n",
    "    p = {}\n",
    "    p['name'] = friend['name']\n",
    "    p['description'] = friend['description']\n",
    "    p['screen_name'] = friend['screen_name']\n",
    "    p['created_at'] = friend['created_at']\n",
    "    p['statuses_count'] = friend['statuses_count']\n",
    "    p['followers_count'] = friend['followers_count']\n",
    "    p['friends_count'] = friend['friends_count']\n",
    "    p['verified'] = friend['verified']\n",
    "    p['id'] = friend['id']\n",
    "    friends_payloads.append(p)\n",
    "    \n",
    "friends_df = pd.DataFrame(friends_payloads)\n",
    "friends_df['created_at'] = pd.to_datetime(friends_df['created_at'])\n",
    "friends_df['created_at'] = friends_df['created_at'].dt.tz_convert(None)\n",
    "friends_df['account_age'] = friends_df['created_at'].apply(lambda x:(datetime.now() - x)/pd.Timedelta(1,'d'))\n",
    "friends_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sample of Twitter accounts, are there any interesting trends in verified accounts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs = plt.subplots(1,4,figsize=(16,4),sharey=True)\n",
    "\n",
    "sb.barplot(x='verified',y='followers_count',data=friends_df,ax=axs[0],estimator=np.mean,errwidth=5)\n",
    "sb.barplot(x='verified',y='friends_count',data=friends_df,ax=axs[1],estimator=np.mean,errwidth=5)\n",
    "sb.barplot(x='verified',y='statuses_count',data=friends_df,ax=axs[2],estimator=np.mean,errwidth=5)\n",
    "sb.barplot(x='verified',y='account_age',data=friends_df,ax=axs[3],estimator=np.mean,errwidth=5)\n",
    "\n",
    "axs[0].set_title('Followers')\n",
    "axs[1].set_title('Friends')\n",
    "axs[2].set_title('Statuses')\n",
    "axs[3].set_title('Account age (days)')\n",
    "\n",
    "# As we'll see below, having more than 5,000 friends could complicate our sampling\n",
    "axs[0].axhline(5000,ls='--',c='k')\n",
    "axs[1].axhline(5000,ls='--',c='k')\n",
    "axs[2].axhline(3200,ls='--',c='k')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylim((1e0,1e8))\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these differences statistically-significant? Let's run some [t-tests](https://en.wikipedia.org/wiki/T-test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "for var in ['followers_count','friends_count','statuses_count','account_age']:\n",
    "    vals1 = friends_df.loc[friends_df['verified'] == True,var]\n",
    "    vals2 = friends_df.loc[friends_df['verified'] == False,var]\n",
    "    test,pvalue = stats.ttest_ind(vals1,vals2)\n",
    "    str_fmt = \"The differences in {0}: t = {1:.2f} \\t p={2:.3f}\"\n",
    "    print(str_fmt.format(var,test,pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use the non-parametric [Mann-Whitney U-test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) since our data is so skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['followers_count','friends_count','statuses_count','account_age']:\n",
    "    vals1 = friends_df.loc[friends_df['verified'] == True,var]\n",
    "    vals2 = friends_df.loc[friends_df['verified'] == False,var]\n",
    "    test,pvalue = stats.mannwhitneyu(vals1,vals2)\n",
    "    str_fmt = \"The differences in {0}: U = {1:,.0f} \\t p = {2:,.3f}\"\n",
    "    print(str_fmt.format(var,test,pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, verified accounts have more followers and are older than non-verified accounts. But they also appear to be more active and have more friends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ego-network\n",
    "We can make a 1.5-step ego-network of the accounts @aoc follows and the accounts each of them follow. Using the `GetFriends` is too \"expensive\" because it cost us 8 API calls to get a single account's followers since it only returns 200 accounts at a time. Twitter also exposes a [get friends/ids](https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-friends-ids) end-point that will return up to 5,000 user IDs per request. The number of requests remains 15 requests per 15-minute window, but we can now get the friend networks for 15 accounts per 15 minutes rather than maybe only 1 or 2. The challenge with this is that we will need to \"rehydrate\" these user IDs at some point.\n",
    "\n",
    "Here, we'll use the \"total_count\" parameter to limit it to 5,000 accounts in case one of these accounts follows thousands of accounts. \n",
    "\n",
    "Store the data in a dictionary keyed by account name and with the list of user IDs integers as values. Initialize with @aoc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.users.lookup(screen_name='joebiden')[0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_d = {'939091':api.friends.ids(user_id='939091',count=5000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.application.rate_limit_status()['resources']['friends']['/friends/ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.fromtimestamp(1616698614)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many accounts have more than 5000 friends? So about 10% of our network will be incomplete if we limit to only a single \"page\" of 5,000 user IDs per follower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt5000_friends_df = friends_df[friends_df['friends_count'] > 5000]\n",
    "print(\"There are {0:,} accounts with more than 5,000 friends.\".format(len(gt5000_friends_df)))\n",
    "gt5000_friends_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are some of these high-friend accounts? Even at 5,000 friends per request, it will still cost you 119 API requests (and thus 119 minutes) to get Barack Obama's 593,000 friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the high-friend accounts to skip\n",
    "gt5000_friends_ids = gt5000_friends_df['id'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop will go through the list of `aoc_friends` (a list of `User` objects) and then get the 5,000 friends' user IDs for each of them. With these rate limits of 15 requests per 15 min, it will take 1417 minutes (23.6 hours) to get our sample of data for AOC's 1,417 friends. You can now start to see the appeal of parallelizing requests!\n",
    "\n",
    "You probably don't want to run this loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each of @joebiden's friend IDs\n",
    "for friend_id in friends_d['939091']['ids']:\n",
    "    \n",
    "    # Check to make sure the friend ID isn't already in the dictionary and is not a high-friend account\n",
    "    if friend_id not in friends_d.keys() and friend_id not in gt5000_friends_ids:\n",
    "        \n",
    "        # Try to get the account's friends\n",
    "        try:\n",
    "            friends_d[friend_id] = api.friends.ids(user_id=friend_id,count=5000)\n",
    "            \n",
    "        # If you get a TwitterError, assume its a rate limit problem\n",
    "        except twitter.TwitterError:\n",
    "            \n",
    "            # Get the current rate limit status\n",
    "            reset_time = api.application.rate_limit_status()['resources']['friends']['/friends/ids']['reset']\n",
    "            \n",
    "            # Wait until the API limit refreshed and add a second for good measure\n",
    "            sleep_time = (datetime.fromtimestamp(reset_time) - datetime.now())/timedelta(seconds=1) + 1\n",
    "            \n",
    "            # Print out to make sure\n",
    "            print(\"At {0}, sleeping for {1} seconds.\".format(datetime.now(),sleep_time))\n",
    "            \n",
    "            # Sleep until our API limit refreshes\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "            # Try to get the friend ID again\n",
    "            friends_d[friend_id] = api.friends.ids(user_id=friend_id,count=5000)\n",
    "            \n",
    "    # Write the friend IDs out to disk after each friend ID\n",
    "    with open('joebiden_friends_ids.json','w') as f:\n",
    "        json.dump(friends_d,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, I've done this scraping for you and saved the results in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('joebiden_friends_ids.json','r') as f:\n",
    "    friends_d = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a network of who follows whom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_l = []\n",
    "\n",
    "# Turn the dictionary into an edgelist\n",
    "for user_id, friend_ids in friends_d.items():\n",
    "    for friend_id in friend_ids['ids']:\n",
    "        friends_l.append((str(user_id),str(friend_id)))\n",
    "        \n",
    "# Turn the list of dictionaries into a DataFrame\n",
    "friends_gdf = pd.DataFrame(friends_l,columns=['user_id','friend_id'])\n",
    "\n",
    "# Get the unique user_ids for AOC's friends\n",
    "unique_friend_ids = friends_gdf['user_id'].unique()\n",
    "\n",
    "# Just keep friends of joebiden in the list\n",
    "# Throw away friends of friends who aren't direct friends of joebiden\n",
    "subset_friends_df = friends_gdf[friends_gdf['friend_id'].isin(unique_friend_ids)]\n",
    "\n",
    "# Print out number of edges remaining\n",
    "print('Edges before: {:,}'.format(len(friends_gdf)),'\\nEdges after: {:,}'.format(len(subset_friends_df)))\n",
    "\n",
    "# Inspect\n",
    "subset_friends_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the numeric user_id back to screen_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_screen_name_map = {str(user['id']):user['screen_name'] for user in friends['users']}\n",
    "ids_to_screen_name_map['939091'] = 'joebiden'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on the [shared audience measure](http://faculty.washington.edu/kstarbi/Stewart_Starbird_Drawing_the_Lines_of_Contention-final.pdf) used by Stewart, *et al.* (2017), I computed [Jaccard coefficients](https://en.wikipedia.org/wiki/Jaccard_index) for the friend sets of each account. The intuituion here is that if two accounts are friends with all the same accounts, their score would be 1 while if two accounts had no friends in common, their score would be 0. This has the benefit of giving us a numerical weight to otherwise binary friend relationships: friend relations are \"stronger\" if they are more strongly embedded in a network with other overlapping friend relations and \"weaker\" if there is less overlap. This requires pair-wise evaluations of $1420*1419=2,014,980$ combinations, which takes about 20 minutes on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_l = []\n",
    "\n",
    "gt5000_friends_ids\n",
    "\n",
    "for f1 in unique_friend_ids:\n",
    "    for f2 in unique_friend_ids:\n",
    "        if f1 != f2 and int(f1) not in gt5000_friends_ids and int(f2) not in gt5000_friends_ids:\n",
    "            try:\n",
    "#                 f1_int = int(f1)\n",
    "#                 f2_int = int(f2)\n",
    "                jaccard = len(set(friends_d[f1]['ids']) & set(friends_d[f2]['ids']))/len(set(friends_d[f1]['ids']) | set(friends_d[f2]['ids']))\n",
    "                jaccard_l.append({'user':f1,'friend':f2,'jaccard':jaccard})\n",
    "            except:\n",
    "                print(f1,f2)\n",
    "                pass\n",
    "            \n",
    "friend_jaccard_df = pd.DataFrame(jaccard_l)[['user','friend','jaccard']]\n",
    "\n",
    "friend_jaccard_df.to_csv('all_friend_jaccard.csv')\n",
    "\n",
    "friend_jaccard_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the `subset_friends_df` with `friend_jaccard_df` using pandas's [`merge`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left-join the subset_friends and friend_jaccard DataFrames\n",
    "friend_el = pd.merge(subset_friends_df,\n",
    "                     friend_jaccard_df,\n",
    "                     left_on=['user_id','friend_id'],\n",
    "                     right_on=['user','friend'],how='left')\n",
    "\n",
    "# Keep a few columns\n",
    "friend_el = friend_el[['user','friend','jaccard']]\n",
    "\n",
    "# Map the user_ids back to screen_names\n",
    "friend_el['user'] = friend_el['user'].apply(str).map(ids_to_screen_name_map)\n",
    "friend_el['friend'] = friend_el['friend'].apply(str).map(ids_to_screen_name_map)\n",
    "\n",
    "# Save to disk\n",
    "# friend_el.to_csv('aoc_friends_edgelist.csv')\n",
    "\n",
    "# Inspect\n",
    "friend_el.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [`networkx`](https://networkx.github.io/documentation/stable/) (that should come with Anaconda by default) to convert this edgelist into a Graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This raw network is very dense: there are about 100 times more edges than nodes. A general heuristic for graph visualization is you want the number of nodes and edges to be about the same order of magnitude to prevent [overplotting](https://www.displayr.com/what-is-overplotting/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dense = nx.from_pandas_edgelist(friend_el,source='user',target='friend',edge_attr='jaccard',create_using=nx.Graph)\n",
    "print(\"There are {0:,} nodes and {1:,} edges.\".format(g_dense.number_of_nodes(),g_dense.number_of_edges()))\n",
    "nx.write_gexf(g_dense,'all_friends.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize. There are better packages for doing this (like Gephi), but let's do something easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(12,12))\n",
    "\n",
    "pos = nx.layout.rescale_layout_dict(nx.layout.spring_layout(g_dense,iterations=100),1.2)\n",
    "\n",
    "nx.draw_networkx_nodes(g_dense,pos,node_size=[i*1e3 for i in nx.degree_centrality(g_dense).values()])\n",
    "nx.draw_networkx_edges(g_dense,pos,width=[d['jaccard']*33 for i,j,d in g_dense.edges(data=True)],alpha=.2)\n",
    "nx.draw_networkx_labels(g_dense,pos,font_size=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the streaming API\n",
    "\n",
    "We can also sit on Twitter's [Streaming API](https://developer.twitter.com/en/docs/tweets/sample-realtime/api-reference/get-statuses-sample) and get a sample of tweets that are produced in real time. The `.GetStreamSample()` method returns a [generator](https://wiki.python.org/moin/Generators), which is an advanced type of object that doesn't store any data *per se* but points to successive locations where you can find data. In this case, the generator points to where we can find the next tweet in the sample. For 10,000 tweets on a stream sampling approximately 1% of live tweets, this may take 2â€“3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the generator\n",
    "stream = twitter.TwitterStream(auth=twitter.OAuth(twitter_keys['access_token_key'],\n",
    "                                         twitter_keys['access_token_secret'],\n",
    "                                         twitter_keys['consumer_key'],\n",
    "                                         twitter_keys['consumer_secret']))\n",
    "\n",
    "tweet_stream = stream.statuses.sample()\n",
    "\n",
    "# Make an empty list to store the tweet statuses\n",
    "stream_list = []\n",
    "\n",
    "# Start iterating through the stream\n",
    "for status in tweet_stream:\n",
    "    \n",
    "    # As long as we have fewer than this many tweets\n",
    "    if len(stream_list) < 1000:\n",
    "        \n",
    "        # And if it's not a delete status request\n",
    "        if 'delete' not in status:\n",
    "        \n",
    "            # Add another tweet to our list\n",
    "            stream_list.append(status)\n",
    "        \n",
    "    # Otherwise stop\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "\"There are {0:,} tweets from the stream.\".format(len(stream_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at one of our statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adapt our previous tweet_cleaner code to turn this JSON data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(status):\n",
    "    payload = {}\n",
    "    payload['screen_name'] = status['user']['screen_name']\n",
    "    payload['created'] = pd.to_datetime(status['created_at'])\n",
    "    payload['retweets'] = status['retweet_count']\n",
    "    payload['favorites'] = status['favorite_count']\n",
    "    payload['id'] = status['id']\n",
    "    payload['reply_screen_name'] = status['in_reply_to_screen_name']\n",
    "    payload['reply_id'] = status['in_reply_to_status_id']\n",
    "    payload['source'] = BeautifulSoup(status['source']).text\n",
    "    payload['lang'] = status['lang']\n",
    "    \n",
    "    if status['place']:\n",
    "        payload['place'] = status['place']['country']\n",
    "    else:\n",
    "        payload['place'] = None\n",
    "\n",
    "    if len(status['entities']['user_mentions']) > 0:\n",
    "        payload['user_mentions'] = '; '.join([m['screen_name'] for m in status['entities']['user_mentions']])\n",
    "    else:\n",
    "        payload['user_mentions'] = None\n",
    "\n",
    "    if len(status['entities']['hashtags']) > 0:\n",
    "        payload['hashtags'] = '; '.join([h['text'] for h in status['entities']['hashtags']])\n",
    "    else:\n",
    "        payload['hashtags'] = None\n",
    "\n",
    "    # If an account retweets another account, we should store that information\n",
    "    if 'retweeted_status' in status:\n",
    "        rt_status = status['retweeted_status']\n",
    "        if 'extended_tweet' in rt_status:\n",
    "            payload['text'] = rt_status['extended_tweet']['full_text']\n",
    "            if len(rt_status['extended_tweet']['entities']['hashtags']) > 0:\n",
    "                payload['hashtags'] = '; '.join([h['text'] for h in rt_status['extended_tweet']['entities']['hashtags']])\n",
    "            else:\n",
    "                payload['hashtags'] = None\n",
    "        else:\n",
    "            try:\n",
    "                payload['text'] = rt_status['text']\n",
    "            except:\n",
    "                payload['text'] = rt_status['full_text']\n",
    "            if len(rt_status['entities']['hashtags']) > 0:\n",
    "                payload['hashtags'] = '; '.join([h['text'] for h in rt_status['entities']['hashtags']])\n",
    "            else:\n",
    "                payload['hashtags'] = None\n",
    "        payload['is_retweet'] = True\n",
    "        payload['retweeted_screen_name'] = rt_status['user']['screen_name']\n",
    "        payload['retweeted_created'] = rt_status['created_at']\n",
    "        payload['retweeted_source'] = BeautifulSoup(rt_status['source']).text\n",
    "        \n",
    "    else:\n",
    "        if status['truncated']:\n",
    "            payload['text'] = status['extended_tweet']['full_text']\n",
    "        else:\n",
    "            try:\n",
    "                payload['text'] = status['text']\n",
    "            except:\n",
    "                payload['text'] = status['full_text']\n",
    "        payload['is_retweet'] = False\n",
    "        payload['retweeted_screen_name'] = False\n",
    "        payload['retweeted_created'] = False\n",
    "        payload['retweeted_source'] = False\n",
    "\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through our list of dictionaries (including the delete stream objects) and flatten the dictionaries out into something we can read into a DataFrame. Include some exception handling that will keep track of which tweets throw errors and prints out the first 50 of those tweet's index position in the `stream_list` for us to diagnose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_statuses_flat = []\n",
    "errors = []\n",
    "\n",
    "for i,status in enumerate(stream_list):\n",
    "    try:\n",
    "        payload = tweet_cleaner(status)\n",
    "        stream_statuses_flat.append(payload)\n",
    "    except:\n",
    "        errors.append(str(i))\n",
    "\n",
    "if len(errors) == 0:\n",
    "    print(\"There were no errors!\")\n",
    "else:\n",
    "    print(\"There were errors at the following indices:\", '; '.join(errors[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make our DataFrame, clean up some columns, and make some new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = pd.DataFrame(stream_statuses_flat)\n",
    "stream_df['created'] = pd.to_datetime(stream_df['created'])\n",
    "stream_df['created'] = stream_df['created'].dt.tz_convert(None)\n",
    "stream_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are people writing their tweets in this sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df['source'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What languages are these tweets in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df['lang'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tweet is geolocated, where is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df['place'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tweets are retweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df['is_retweet'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tweets are replies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df['reply_id'].notnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which users are getting a lot of retweets right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df['retweeted_screen_name'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered streams\n",
    "\n",
    "We can also filter the tweets in the stream. Here we only get tweets mentioning \"Biden\" and that have been auto-classified as written in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the generator\n",
    "filtered_stream = stream.statuses.filter(track='Biden',languages='en')\n",
    "\n",
    "# Make an empty list to store the tweet statuses\n",
    "filtered_stream_list = []\n",
    "\n",
    "# What time did we start?\n",
    "start = time.time()\n",
    "\n",
    "# Start iterating through the stream\n",
    "for status in filtered_stream:\n",
    "    \n",
    "    # As long as we have fewer than this many tweets\n",
    "    if len(filtered_stream_list) < 1000:\n",
    "        \n",
    "        # And if it's not a delete status request\n",
    "        if 'delete' not in status:\n",
    "        \n",
    "            # Add another tweet to our list\n",
    "            filtered_stream_list.append(status)\n",
    "        \n",
    "    # Otherwise stop\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# What time did we stop?\n",
    "stop = time.time()\n",
    "elapsed = stop - start\n",
    "\n",
    "\"There are {0:,} tweets from the stream after {1:.0f} seconds.\".format(len(filtered_stream_list),elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean this up into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stream_statuses_flat = []\n",
    "filtered_errors = []\n",
    "\n",
    "for i,status in enumerate(filtered_stream_list):\n",
    "    try:\n",
    "        payload = tweet_cleaner(status)\n",
    "        filtered_stream_statuses_flat.append(payload)\n",
    "    except:\n",
    "        filtered_errors.append(str(i))\n",
    "\n",
    "if len(filtered_errors) == 0:\n",
    "    print(\"There were no errors!\")\n",
    "else:\n",
    "    print(\"There were errors at the following indices:\", '; '.join(filtered_errors[:50]))\n",
    "    \n",
    "filtered_stream_df = pd.DataFrame(filtered_stream_statuses_flat)\n",
    "filtered_stream_df['created'] = pd.to_datetime(filtered_stream_df['created'])\n",
    "filtered_stream_df['created'] = filtered_stream_df['created'].dt.tz_convert(None)\n",
    "filtered_stream_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the sentiment of the tweets in this filtered s ample and plot the distribution of their sentiment values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sentiment scores\n",
    "filtered_stream_df['sentiment'] = filtered_stream_df['text'].apply(lambda x:sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Plot the distribution\n",
    "filtered_stream_df['sentiment'].plot(kind='hist',bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many retweets in this sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stream_df['is_retweet'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the higher fraction of retweets, who is being retweeted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stream_df['retweeted_screen_name'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search API\n",
    "\n",
    "Twitter's [search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets) provides an endpoint to search for tweets matching a query for terms, accounts, hashtags, language, locations, and date ranges. This API endpoint has a rate limit of 180 requests per 15-minute window with 100 statuses per request: or 18,000 statuses per window or 72,000 statuses per hour.\n",
    "\n",
    "You can explore some of the search functionality through Twitter's [advanced search interface](https://twitter.com/search-advanced). Note that the [standard search API](https://developer.twitter.com/en/docs/tweets/search/overview/standard) only provides a limited access to sample of tweets in the past 7 days, you'll need to pay more to access [historical APIs](https://developer.twitter.com/en/docs/tutorials/choosing-historical-api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = api.search.tweets(q='boulder',\n",
    "                          count=100,\n",
    "                          lang='en',\n",
    "                          result_type='recent',\n",
    "                          tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through these 100 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_statuses_flat = []\n",
    "search_errors = []\n",
    "\n",
    "for i,status in enumerate(query['statuses']):\n",
    "    try:\n",
    "        payload = tweet_cleaner(status)\n",
    "        search_statuses_flat.append(payload)\n",
    "    except:\n",
    "        search_errors.append(str(i))\n",
    "\n",
    "if len(search_errors) == 0:\n",
    "    print(\"There were no errors!\")\n",
    "else:\n",
    "    print(\"There were errors at the following indices:\", '; '.join(search_errors[:50]))\n",
    "    \n",
    "search_df = pd.DataFrame(search_statuses_flat)\n",
    "search_df['created'] = pd.to_datetime(search_df['created'])\n",
    "search_df['created'] = search_df['created'].dt.tz_convert(None)\n",
    "search_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop to try to get more tweets. The `query` dictionary includes a sub-dictionary under the \"search_metadata\" key that includes information about paginating to find the next set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tweets = []\n",
    "\n",
    "while True:\n",
    "    # When to stop?\n",
    "    if len(search_tweets) == 2500:\n",
    "        break\n",
    "    \n",
    "    # Get the first set of tweets\n",
    "    if len(search_tweets) == 0:\n",
    "        query = api.search.tweets(q='boulder',\n",
    "                          count=100,\n",
    "                          lang='en',\n",
    "                          result_type='recent',\n",
    "                          tweet_mode='extended')\n",
    "        search_tweets += query['statuses']\n",
    "        \n",
    "    # Keep getting tweets\n",
    "    else:\n",
    "        # Find the last tweet to use as a max_id\n",
    "        max_id = search_tweets[-1]['id']\n",
    "        \n",
    "        # Get the next set of tweets\n",
    "        query = api.search.tweets(q='boulder',\n",
    "                                  count=100,\n",
    "                                  lang='en',\n",
    "                                  result_type='recent',\n",
    "                                  tweet_mode='extended',\n",
    "                                  max_id = max_id - 1)\n",
    "        \n",
    "        # Add them to the list of tweets\n",
    "        search_tweets += query['statuses']\n",
    "        \n",
    "print(\"There are {0:,} tweets in the collection.\".format(len(search_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_statuses_flat = []\n",
    "search_errors = []\n",
    "\n",
    "for i,status in enumerate(search_tweets):\n",
    "    try:\n",
    "        payload = tweet_cleaner(status)\n",
    "        search_statuses_flat.append(payload)\n",
    "    except:\n",
    "        search_errors.append(str(i))\n",
    "\n",
    "if len(search_errors) == 0:\n",
    "    print(\"There were no errors!\")\n",
    "else:\n",
    "    print(\"There were errors at the following indices:\", '; '.join(search_errors[:50]))\n",
    "    \n",
    "search_df = pd.DataFrame(search_statuses_flat)\n",
    "search_df['created'] = pd.to_datetime(search_df['created'])\n",
    "search_df['created'] = search_df['created'].dt.tz_convert(None)\n",
    "search_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sentiment scores\n",
    "search_df['sentiment'] = search_df['text'].apply(lambda x:sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Plot the distribution\n",
    "search_df['sentiment'].plot(kind='hist',bins=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
