{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Data Scraping\n",
    "\n",
    "[Spring 2019 ITSS Mini-Course](https://www.colorado.edu/cartss/programs/interdisciplinary-training-social-sciences-itss/mini-course-web-data-scraping) — ARSC 5040  \n",
    "[Brian C. Keegan, Ph.D.](http://brianckeegan.com/)  \n",
    "[Assistant Professor, Department of Information Science](https://www.colorado.edu/cmci/people/information-science/brian-c-keegan)  \n",
    "University of Colorado Boulder  \n",
    "\n",
    "Copyright and distributed under an [MIT License](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Class outline\n",
    "\n",
    "* **Week 1**: Introduction to Jupyter, browser console, structured data, ethical considerations\n",
    "* **Week 2**: Scraping HTML with `requests` and `BeautifulSoup`\n",
    "* **Week 3**: Scraping an API with `requests` and `json`, Wikipedia\n",
    "* **Week 4**: Scraping web data with Selenium and using the Internet Archive API\n",
    "* **Week 5**: Scraping data from Twitter\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This course will draw on resources built by myself and [Allison Morgan](https://allisonmorgan.github.io/) for the [2018 Summer Institute for Computational Social Science](https://github.com/allisonmorgan/sicss_boulder), which were in turn derived from [other resources](https://github.com/simonmunzert/web-scraping-with-r-extended-edition) developed by [Simon Munzert](http://simonmunzert.github.io/) and [Chris Bail](http://www.chrisbail.net/). \n",
    "\n",
    "Thank you also to Professors [Bhuvana Narasimhan](https://www.colorado.edu/linguistics/bhuvana-narasimhan) and [Stefanie Mollborn](https://behavioralscience.colorado.edu/person/stefanie-mollborn) for coordinating the ITSS seminars.\n",
    "\n",
    "## Class 4 goals\n",
    "\n",
    "* Sharing accomplishments and challenges with last week's material\n",
    "* Using Selenium to interact with websites\n",
    "* Implementing a screen-scraper with Selenium \n",
    "* Ethics of spoofing headers, screen scraping, and parallelizing API requests\n",
    "* Using Internet Archive API to find historical web pages\n",
    "* Retrieving and parsing Internet Archive pages\n",
    "\n",
    "Start with our usual suspect packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets us talk to servers on the web\n",
    "import requests\n",
    "\n",
    "# Parsing HTML magic\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Will be helful for converting between timestamps\n",
    "from datetime import datetime\n",
    "\n",
    "# We want to sleep from time-to-time to avoid overwhelming another server\n",
    "import time\n",
    "\n",
    "from urllib.parse import quote, unquote\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block of code below will only work once you've installed Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our interface to a real-life web browser... won't import until you install!\n",
    "import selenium.webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Selenium\n",
    "\n",
    "This is a non-trivial process: you will need to (1) install the Python bindings for Selenium, (2) download a web driver to interface with a web browser, and (3) configure Selenium to recognize your web driver. Follow the installation instructions in the documentation [here](https://selenium-python.readthedocs.io/installation.html) (you won't need the Selenium server).\n",
    "\n",
    "1. Install the Python bindings for Selenium. Go to your Anaconda terminal window, type in this command, and agree to whatever the package manager wants to install or update.\n",
    "\n",
    "`conda install selenium`\n",
    "\n",
    "2. Download the driver(s) for the web browser you want to use from the [links on the Selenium documentation](https://selenium-python.readthedocs.io/installation.html). If you use a Chrome browser, download the Chrome driver. Note that the Safari driver will not work on PCs and the Edge driver will not work on Macs. \n",
    "\n",
    "3. You will need to unzip the file and move the executable to the same directory where you are running this notebook. Make a note of the path to this directory.\n",
    "\n",
    "### Using Selenium to control a web browser\n",
    "The `driver` object we create is a connection from this Python environment out to the browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Chrome driver for my PC -- yours is likely very different\n",
    "# pc_path = 'E:/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver.exe'\n",
    "# driver = selenium.webdriver.Chrome(executable_path=pc_path)\n",
    "\n",
    "# Path to the Chrome driver for my Mac -- yours is likely very different\n",
    "mac_path = '/Users/briankeegan/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver'\n",
    "driver = selenium.webdriver.Chrome(executable_path=mac_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single line of code will open a new browser window and will request the \"xkcd\" homepage.\n",
    "\n",
    "Your computer's security protocols may vigorously protest because you are launching a program that is controlled by another process/program. You will need to dismiss these warnings in order to proceed. Whether and how to do that will vary considerably across PCs and Macs, the kinds of permissions your account has on this operating system, and other security measures employed by your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://xkcd.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Classes 01 and 02, we used `BeautifulSoup` to turn HTML and XML into a data structure that we could search and access using Python-like syntax. With Selenium we use a standard called \"XPath\" to navigate through an HTML document: [this is the official tutorial](https://www.w3schools.com/xml/xpath_syntax.asp) for working with XPath. The syntax is different, but the intuition is similar: we can find a parent node by its attribute (class, id, *etc*.) and then navigate down the tree to its children.\n",
    "\n",
    "The XPath below has the following elements in sequence\n",
    "* `//` — Select all nodes that match the selection\n",
    "* `[@id=\"middleContainer\"]` — find the element that has a \"middleContainer\" id.\n",
    "* `/ul[2]` — select the second `<ul>` element underneath the `<div id=\"middleContainer\">`\n",
    "* `/li[3]` — select the third `<li>` element \n",
    "* `/a` — select the a element\n",
    "\n",
    "The combined XPath string `//*[@id=\"middleContainer\"]/ul[1]/li[3]/a` is like a \"file directory\" that (hopefully!) points to the hyperlink button that takes us to a random xkcd comic. With the directions to this button, we can have the web browser \"click\" the \"Random\" button beneath the comic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the 'random' buttom\n",
    "element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[2]/li[3]/a')\n",
    "\n",
    "# Once we've found it, now click it\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the attributes of different parts of the web page. xkcd is famous for its \"hidden messages\" inside the image alt-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alttext_element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "alttext_element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write a simple loop to click on the random button five times and print the alt-text from each of those pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(5):\n",
    "    random_element = driver.find_element_by_xpath('//*[@id=\"middleContainer\"]/ul[2]/li[3]/a')\n",
    "    random_element.click()\n",
    "    \n",
    "    alttext_element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "    print('\\n',alttext_element.get_attribute(\"title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done playing with your programmable web browser, make sure to close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the connection to the web browser closed, any of the functions like `find_element_by_xpath`, `click()`, *etc*. will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alttext_element = driver.find_element_by_xpath('//*[@id=\"comic\"]/img')\n",
    "alttext_element.get_attribute(\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just about any operation you do in a web browser can be automated with Selenium: scrolling, clicking, completing forms, moving between tabs/windows, handling pop-ups, navigating back and forward, handling cookies, *etc*. Learn more about the functionality with tutorials and other resources in the [Selenium documentation](https://selenium-python.readthedocs.io/navigating.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Start your driver again and get the xkcd homepage.\n",
    "\n",
    "1. Change the XPath to click on the \"Prev\" button above the comic.\n",
    "2. Change the XPath to search for the \"comicNav\" class instead of the \"middleContainer\" id.\n",
    "3. Change the XPath to click on the \"About\" button in the upper-left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical web scraping\n",
    "\n",
    "James Densmore has a nice summary of [practices for ethical web scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01):\n",
    "\n",
    "> * If you have a public API that provides the data I’m looking for, I’ll use it and avoid scraping all together.\n",
    "> * I will always provide a User Agent string that makes my intentions clear and provides a way for you to contact me with questions or concerns.\n",
    "> * I will request data at a reasonable rate. I will strive to never be confused for a DDoS attack.\n",
    "> * I will only save the data I absolutely need from your page. If all I need it OpenGraph meta-data, that’s all I’ll keep.\n",
    "> * I will respect any content I do keep. I’ll never pass it off as my own.\n",
    "> * I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
    "> * I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
    "> * I will scrape for the purpose of creating new value from the data, not to duplicate it.\n",
    "\n",
    "Some other important components of ethical web scraping practices [include](http://robertorocha.info/on-the-ethics-of-web-scraping/):\n",
    "\n",
    "* Reading the Terms of Service and Privacy Policies for the site's rules on scraping.\n",
    "* Inspecting the robots.txt file for rules about what pages can be scraped, indexed, *etc*.\n",
    "* Be gentle on smaller websites by running during off-peak hours and spacing out requests.\n",
    "* Identify yourself by name and email in your User-Agent strings\n",
    "\n",
    "What does a robots.txt file look like? Here is CNN's. It helpfull provides a sitemap to the robot to get other pages, it allows all kinds of User-agents, and disallows crawling of pages in specific directories (ads, polls, tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.get('https://www.cnn.com/robots.txt').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are scraping websites, it is a good idea to include your contact information as a custom User-Agent string so that the webmaster can get in contact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_header = {'User-Agent':'Python research tool by Brian Keegan, brian.keegan@colorado.edu'}\n",
    "\n",
    "request = requests.get('https://www.cnn.com',headers=contact_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adverse consequences of web scraping include:\n",
    "* Compromising the privacy and integrity of individual users' data\n",
    "* Damaging a web server with too many requests\n",
    "* Denying access to the web service to other authorized users\n",
    "* Infringing on copyrighted material\n",
    "* Damaging the business value of a web site\n",
    "\n",
    "[Amanda Bee](http://velociraptor.info/) compiled [a nice set of examples](https://github.com/amandabee/scraping-for-journalists/wiki/Reporting-Examples) of data journalists using web scraping for their reporting. There are some ethical justifications for violating a site's terms of service to scrape data:\n",
    "* Obtaining data for the public interest from official statements, government reports, *etc*.\n",
    "* Conducting audit studies (as long as these are responsibly designed and pre-cleared)\n",
    "* The data is unavailable from APIs, FOIA requests, and other reports\n",
    "\n",
    "[Sophie Chou](http://sophiechou.com/) made this nice [decision flow-chart](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/) of whether to build a scraper or not from a NICAR panel in 2016:\n",
    "\n",
    "![Should you build a scraper flowchart](http://www.storybench.org/wp-content/uploads/2016/04/flowchart_final.jpeg)\n",
    "\n",
    "Why is there a \"Talk to a lawyer?\" outcome at the bottom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Fraud and Abuse Act\n",
    "\n",
    "The [Computer Fraud and Abuse Act](https://en.wikipedia.org/wiki/Computer_Fraud_and_Abuse_Act) was passed in 1984, [in large part due to](https://www.cnet.com/news/from-wargames-to-aaron-swartz-how-u-s-anti-hacking-law-went-astray/) the 1983 film [WarGames](https://en.wikipedia.org/wiki/WarGames) starring Matthew Broderick. A plain reading of the text of the law ([18 U.S.C. § 1030](https://www.law.cornell.edu/uscode/text/18/1030)) criminalizes just about any form of web scraping:\n",
    "\n",
    "> * Whoever intentionally accesses a computer without authorization or exceeds authorized access, and thereby obtains… information from any protected computer;\n",
    "> * knowingly causes the transmission of a program, information, code, or command, and as a result of such conduct, intentionally causes damage without authorization, to a protected computer;\n",
    "> * the term “exceeds authorized access” means to access a computer with authorization and to use such access to obtain or alter information in the computer that the accesser is not entitled so to obtain or alter;\n",
    "> * the term “damage” means any impairment to the integrity or availability of data, a program, a system, or information;\n",
    "> * the term “protected computer” means a computer which is used in or affecting interstate or foreign commerce or communication, including a computer located outside the United States that is used in a manner that affects interstate or foreign commerce or communication of the United States;\n",
    "\n",
    "Violators can be fined and jailed under a misdemeanor charge for up to 1 year for the first violation and jailed up to 10 years under a felony charge for repeated violations.\n",
    "\n",
    "This law has a [chilling effect](https://en.wikipedia.org/wiki/Chilling_effect) on many forms of research, journalism, and other forms of protected speech. The CFAA has been used by federal prosecutors to bring federal felony charges against programmers, journalists, and activists. In 2011, programmer and hacktivist [Aaron Swartz](https://en.wikipedia.org/wiki/Aaron_Swartz) (who contributed to the development of RSS, Markdown, Creative Commons, and Reddit) was [arrested and charged](https://en.wikipedia.org/wiki/United_States_v._Swartz) with violating the CFAA for downloading several million PDFs from JSTOR over MIT's network. The [decision to prosecute was unusual](https://www.huffingtonpost.com/2013/03/13/aaron-swartz-prosecutorial-misconduct_n_2867529.html). Facing 35 years of imprisonment and over $1 million in fines under the CFAA, Swartz committed suicide on January 11, 2013.\n",
    "\n",
    "In 2016, four computer science researchers and the publisher of *The Intercept* who all use scraping techniques to run experiments to measure bias and discrimination in web content [filed suit with the ACLU](https://www.aclu.org/cases/sandvig-v-sessions-challenge-cfaa-prohibition-uncovering-racial-discrimination-online) against the U.S. Government: *Sandvig v. Sessions*. Their research involves creating multiple fake accounts, providing inaccurate information to websites, using automated tools to record publicly-available data, and other scraping techniques. In March 2018, the [D.C. Circuit Court ruled](https://www.aclu.org/news/judge-allows-aclu-case-challenging-law-preventing-studies-big-data-discrimination-proceed) two of the plantiffs have standing to sue and the case is currently being prepared for trial.\n",
    "\n",
    "### Warning\n",
    "\n",
    "The code we will write and execute below will repeatedly violate Twitter's [Terms of Service](https://twitter.com/en/tos) (\"scraping the Services without the prior consent of Twitter is expressly prohibited\") for retrieving information from the platform. In effect, we will transmit code in excess of our authorized access and potentially cause damage, in order to obtain information from a protected computer. \n",
    "\n",
    "We will do this in order to obtain public statements made by goverment officials acting in their official capacity because this data is otherwise unavailable for retrieval from Twitter. There is an interesting body of emerging legal precedent treating elected officials' use of Twitter as a public forum: [*Knight First Amendment Institute v. Trump*](https://en.wikipedia.org/wiki/Knight_First_Amendment_Institute_v._Trump) established that [the President may not block other Twitter users](https://www.courtlistener.com/docket/6087955/72/knight-first-amendment-institute-at-columbia-university-v-trump/):\n",
    "\n",
    "> * \"We hold that portions of the @realDonaldTrump account -- the “interactive space” where Twitter users may directly engage with the content of the President’s tweets -- are properly analyzed under the “public forum” doctrines set forth by the Supreme Court, that such space is a designated public forum...\"\n",
    "> * \"we nonetheless conclude that the extent to which the President and Scavino can, and do, exercise control over aspects of the @realDonaldTrump account are sufficient to establish the government-control element as to the content of the tweets sent by the @realDonaldTrump account, the timeline compiling those tweets, and the interactive space associated with each of those tweets.\"\n",
    "> * \"Because a Twitter user lacks control over the comment thread beyond the control exercised over first-order replies through blocking, the comment threads -- as distinguished from the content of tweets sent by @realDonaldTrump, the @realDonaldTrump timeline, and the interactive space associated with each tweet -- do not meet the threshold criterion for being a forum.\"\n",
    "> * \"the account’s timeline, which “displays all tweets generated by the [account]”... all of which is government speech.\"\n",
    "\n",
    "I would advise you against using these tools and approaches without a similarly clear public interest rationale and jurisprudence linking behavior to public forum doctrines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen-scraping a Twitter ego network with Selenium\n",
    "\n",
    "I am adapting a [tutorial by Shawn Wang](https://dev.to/swyx/scraping-my-twitter-social-graph-with-python-and-selenium--hn8) on scraping a Twitter graph with Python and Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Chrome driver for my PC -- yours is likely very different\n",
    "# driver = selenium.webdriver.Chrome(executable_path='E:/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver.exe')\n",
    "\n",
    "# Path to the Chrome driver for my Mac -- yours is likely very different\n",
    "driver = selenium.webdriver.Chrome(executable_path='/Users/briankeegan/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver')\n",
    "\n",
    "driver.get('https://www.twitter.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually log in to your Twitter account through the driver page.\n",
    "\n",
    "Then go to the \"followings\" (or followees, also called \"friends\" in the Twitter API) of an account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://twitter.com/realDonaldTrump/following')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of this Notebook's writing, the \"realDonaldTrump\" account followed 45 other accounts. Depending on the resolution of your display, size of the window, *etc*. there may only be 10–20 accounts visible. We can scroll to see the rest of these accounts programatically.\n",
    "\n",
    "Run this cell a few times to keep scrolling to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the HTML of the web page in the browser back to Python and turn it into soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about each follower lives inside a `<div \"data-item-type\":\"user\">` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_divs = soup.body.find_all('div', attrs={'data-item-type':'user'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `user_div` (at the time of this writing) was for the `@VP` account.\n",
    "\n",
    "Where does the Twitter account handle live in the HTTP document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_divs[0].div['data-screen-name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the name of the Twitter account live in the HTTP document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_divs[0].find_all('a',{'class':'fullname'})[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the bio live in the HTTP document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_divs[0].p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all the pieces together now: loop through each `user_div` and pull out the relevant information to store as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the followings data\n",
    "following_graph_alters = []\n",
    "\n",
    "# Loop through each user_div\n",
    "for ud in user_divs:\n",
    "    \n",
    "    # Create an empty alter dictionary to fill with the handle, name, bio for each user\n",
    "    alter = {}\n",
    "    \n",
    "    # Get the formal account handle\n",
    "    alter['Screen Name'] = ud.div['data-screen-name']\n",
    "    \n",
    "    # Get the displayed name\n",
    "    alter['Display Name'] = ud.find_all('a',{'class':'fullname'})[0].text.strip()\n",
    "    \n",
    "    # Get the biography\n",
    "    alter['Bio'] = ud.p.text\n",
    "    \n",
    "    # Add the alter to the list\n",
    "    following_graph_alters.append(alter)\n",
    "    \n",
    "# Turn the list into a DataFrame\n",
    "following_graph_df = pd.DataFrame(following_graph_alters)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "following_graph_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen-scraping a Twitter account's timeline of tweets\n",
    "\n",
    "Go to the Twitter account for the White House."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://twitter.com/WhiteHouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the source of the page from the browser and soup-ify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of tweet objects currently on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = soup.find_all('div',{'class':'original-tweet'})\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scroll to the bottom of the page, get the source again, parse out the tweets, and count them. On the browser, window size, resolution, *etc*. I'm using, I got 20 more tweets with a single scroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(raw)\n",
    "tweets = soup.find_all('div',{'class':'original-tweet'})\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen name\n",
    "tweets[0]['data-screen-name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Name\n",
    "tweets[0]['data-name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet ID\n",
    "tweets[0]['data-tweet-id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp\n",
    "tweets[0].find('a',{'class':'tweet-timestamp'}).span['data-time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a UNIX time code, also known as the \"UNIX epoch\", or the number of seconds since midnight on January 1, 1970. Because it is a common way to store data, `datetime` provides a way to convert it into a meaningful timestamp: `utcfromtimestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.utcfromtimestamp(1549143559))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text of the tweet\n",
    "tweets[0].find('div',{'class':'js-tweet-text-container'}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replies\n",
    "tweets[0].find_all('span',{'class':'ProfileTweet-actionCount'})[0]['data-tweet-stat-count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retweets\n",
    "tweets[0].find_all('span',{'class':'ProfileTweet-actionCount'})[1]['data-tweet-stat-count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Favorites\n",
    "tweets[0].find_all('span',{'class':'ProfileTweet-actionCount'})[2]['data-tweet-stat-count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that extracts all this information from each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_timeline_parser(tweet):\n",
    "    payload = {}\n",
    "    \n",
    "    payload['Screen name'] = tweet['data-screen-name']\n",
    "    payload[\"Display name\"] = tweet['data-name']\n",
    "    payload['TweetID'] = tweet['data-tweet-id']\n",
    "    payload['Timestamp'] = tweet.find('a',{'class':'tweet-timestamp'}).span['data-time']\n",
    "    payload['Text'] = tweet.find('div',{'class':'js-tweet-text-container'}).text.strip()\n",
    "    payload['Replies'] = int(tweet.find_all('span',{'class':'ProfileTweet-actionCount'})[0]['data-tweet-stat-count'])\n",
    "    payload['Retweets'] = int(tweet.find_all('span',{'class':'ProfileTweet-actionCount'})[1]['data-tweet-stat-count'])\n",
    "    payload['Favorites'] = int(tweet.find_all('span',{'class':'ProfileTweet-actionCount'})[2]['data-tweet-stat-count'])\n",
    "    \n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    parsed_tweet = tweet_timeline_parser(tweet)\n",
    "    parsed_tweets.append(parsed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first three tweets that we parsed from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_tweets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are this many tweets on the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_count = soup.find('a',{'class':'ProfileNav-stat'}).find_all('span')[-1]['data-count']\n",
    "tweet_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some rough math, 6696 divided by 20 tweets per scroll means we need to do 335 scrolls to get the whole timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(tweet_count)/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with 10 scrolls and see whether things are still working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scroll in range(10):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in the data and parse out the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the browser\n",
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "# Soup-ify\n",
    "soup = BeautifulSoup(raw)\n",
    "\n",
    "# Find all the tweets\n",
    "tweets = soup.find_all('div',{'class':'original-tweet'})\n",
    "\n",
    "# Create the container\n",
    "parsed_tweets = []\n",
    "\n",
    "# Try to parse the tweets\n",
    "for tweet in tweets:\n",
    "    parsed_tweet = tweet_timeline_parser(tweet)\n",
    "    parsed_tweets.append(parsed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tweets were parsed out after 10 scrolls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to scroll until we can't, logging how many scrolls we went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the webpage\n",
    "driver.get('https://twitter.com/WhiteHouse')\n",
    "\n",
    "# Initialize the scroll counter and current page height\n",
    "scroll_counter = 0\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Start a loop\n",
    "while True:\n",
    "    \n",
    "    # Print out our progress every 10th scroll\n",
    "    if scroll_counter > 0 and scroll_counter % 10 == 0:\n",
    "        print(\"This is scroll: {0}\".format(scroll_counter))\n",
    "    \n",
    "    # Sleep for 2 seconds in between scrolls\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Get the current height of the page\n",
    "    current_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    \n",
    "    # If the current page height is the same as the previous page height, we can't scroll anymore\n",
    "    if current_height == last_height:\n",
    "        break\n",
    "    \n",
    "    # Scroll to the bottom again\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Increment our scroll counter\n",
    "    scroll_counter += 1\n",
    "    \n",
    "    # Update the height of the page\n",
    "    last_height = current_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the browser\n",
    "raw = driver.page_source.encode('utf-8')\n",
    "\n",
    "# Soup-ify\n",
    "soup = BeautifulSoup(raw)\n",
    "\n",
    "# Find all the tweets\n",
    "tweets = soup.find_all('div',{'class':'original-tweet'})\n",
    "\n",
    "# Create the container\n",
    "parsed_tweets = []\n",
    "\n",
    "# Try to parse the tweets\n",
    "for tweet in tweets:\n",
    "    parsed_tweet = tweet_timeline_parser(tweet)\n",
    "    parsed_tweets.append(parsed_tweet)\n",
    "    \n",
    "print(len(parsed_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn our `parsed_tweets` into a DataFrame for saving to CSV or doing visualization, *etc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(parsed_tweets).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrolling through a timeline and parsing the tweets Twitter serves up—until it doesn't—has many good faith assumptions as a model of data scraping that runs afoul of the language in Twitter's Terms of Service: if I am willing to interface like a human user, then when Twitter limits a human user we also stop collecting data.\n",
    "\n",
    "But we really want all 6,700 of those White House tweets, not just the most recent 831. Twitter's [get statuses/user_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html) API will only return up to 3,200 of a Twitter user's most recent tweets. While we could get the `@WhiteHouse` account's most recent 3,200 tweets, we would not be able to retrieve the first 3,500 tweets. We are going to use Twitter's search functionality to get these first `@WhiteHouse` tweets instead. \n",
    "\n",
    "This *significantly* escalates the burden of proof on the researcher to demonstrate that this violation of Twitter's Terms of Service is ethical. In this specific case, I will argue that violating Twitter's Terms of Service can be justified by the greater importance of being able to build an archive of public statements by government officials that are not otherwise available. However, using this approach to scrape private users' timeline would raise significant ethical concerns about violating a platform's Terms of Service and users' reasonable expectations about privacy and the availability of their data.\n",
    "\n",
    "### Screen scraping\n",
    "\n",
    "We can use Twitter's search functionality to find all the tweet from an account since or until a date and scroll to get all the data. In practice, you can only get up to approximately 9,999 tweets with this approach. \n",
    "\n",
    "First, we'll make a `query_params` dictionary with the name of the account, a start date, and a stop date. Here's we will only do the `@WhiteHouse` tweets from the first year of the Trump administration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the query params\n",
    "query_params = {}\n",
    "query_params['from'] = 'WhiteHouse'\n",
    "query_params['since'] = '2017-01-20'\n",
    "query_params['until'] = '2018-01-20'\n",
    "\n",
    "# Pass the params into a string and quote to format it properly\n",
    "query_params_quoted = quote(\"from:{from} since:{since} until:{until}\".format(**query_params))\n",
    "\n",
    "# Add the quoted query params into the URL\n",
    "query_url = \"https://twitter.com/search?f=tweets&q={0}&src=typd\".format(query_params_quoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the page and scroll to the bottom, preserving the logging functionality to keep track of how far along we are and when problems occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the web page from the URL\n",
    "driver.get(query_url)\n",
    "\n",
    "# Repeat the scrolling until the end\n",
    "scroll_counter = 0\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "while True:\n",
    "    if scroll_counter > 0 and scroll_counter % 10 == 0:\n",
    "        print(\"This is scroll: {0}\".format(scroll_counter))\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    current_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    \n",
    "    if current_height == last_height:\n",
    "        break\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    scroll_counter += 1\n",
    "    last_height = current_height\n",
    "    \n",
    "print(\"It took {:,} scrolls to reach the end.\".format(scroll_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've finished scrolling to load all the data, we can parse it. This may take a while; the `raw` HTML code after all that scrolling is close to 20 million characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source after scrolling and soup-ify\n",
    "raw = driver.page_source.encode('utf-8')\n",
    "print(\"There are {0:,} characters in the raw HTML.\".format(len(raw)))\n",
    "\n",
    "soup = BeautifulSoup(raw)\n",
    "\n",
    "# Find all the tweets\n",
    "tweets = soup.find_all('div',{'class':'original-tweet'})\n",
    "\n",
    "# Create the container\n",
    "parsed_tweets = []\n",
    "\n",
    "# Try to parse the tweets\n",
    "for tweet in tweets:\n",
    "    parsed_tweet = tweet_timeline_parser(tweet)\n",
    "    parsed_tweets.append(parsed_tweet)\n",
    "    \n",
    "print(\"There are {0:,} parsed tweets.\".format(len(parsed_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quit the driver, which closes the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the tweets into a DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_tweets_df = pd.DataFrame(parsed_tweets)\n",
    "\n",
    "# Replace the UTC timestamp with a more usable timestamp\n",
    "historical_tweets_df['Timestamp'] = historical_tweets_df['Timestamp'].apply(lambda x:datetime.utcfromtimestamp(int(x)))\n",
    "\n",
    "# Inspect\n",
    "historical_tweets_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a basic scatterplot of the relationship between replies and retweets. By ocular inspection, there is a pretty strong correlation between the number of retweets and replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = historical_tweets_df.plot(x='Replies',y='Retweets',kind='scatter',logx=True,logy=True,s=5)\n",
    "ax.set_xlim((1e1,1e5))\n",
    "ax.set_ylim((1e1,1e5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoofing headers\n",
    "\n",
    "When we use `requests` to get data from other web servers, each of the get requests carries some meta-data about ourselves, called [headers](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html). These headers tell the server what kind of web browser we are, what kinds of data we can receive, *etc*. so that the server can reply with properly-formatted information. \n",
    "\n",
    "But it is also possible for the server to understand a request and refuse to fulfill it, known as a [HTTP 403 error](https://en.wikipedia.org/wiki/HTTP_403). A server's refusal to fulfill a client's request can often be traced back to the identity a client presents through its headers or a client lacking authorization to access the data (*i.e.*, you need to authenticate with the website first). In the case of `requests`, its `get` request includes default header information that identifies it as a Python script rather than a human-driven web browser.\n",
    "\n",
    "Let's make a request for an article from the NYTimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_response = requests.get('https://www.nytimes.com/2019/02/03/us/politics/trump-interview-mueller.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the headers we sent with this request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honest_response.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the 'User-Agent' string identifies this request as originating from the \"python-requests/2.21.0\" program, rather than a typical web browser. Some web servers will be configured to inspect the headers of incoming requests and refuse requests unless they are actual web browsers.\n",
    "\n",
    "We can often circumvent these filters by sending alternative headers that claim to be from a web browser as a part of our `requests.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with spoofed headers for the User-Agent\n",
    "spoofed_headers = {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "\n",
    "# Make the request with the \n",
    "nytimes_url = 'https://www.nytimes.com/2019/02/03/us/politics/trump-interview-mueller.html'\n",
    "spoofed_response = requests.get(nytimes_url,headers=spoofed_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the get request we sent to the NYTimes web server now includes the spoofed \"User-Agent\" string we wrote that claims our request is from a web browser. The server should now return the data we requested, even though we are not who we claimed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoofed_response.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had trouble finding a website that refused \"python-requests\" connections automatically (*e.g.*, Amazon, NYTimes, etc.), but you will likely find some along the way. \n",
    "\n",
    "Spoofing headers to conceal the identity of your client to a web server is another example of how technological capabilities can overtake ethical responsibilities. The owners of a web server may have good reasons for refusing to serve content to non-web browsers (copyright, privacy, business model, *etc*.). Misrepresenting your identity to extract this data should only be done if the risks to others are small, the benefits are in the public interest, there are no other alternatives for obtaining the data, *etc*. \n",
    "\n",
    "There can be *very* real consequences for spoofing headers. Because it is such a common and relatively trivial method for circumventing server security settings, making repeated spoofed requests could result in your IP address or an IP address range (worst case, the entire university) being blocked from making requests to the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing requests\n",
    "\n",
    "A third web scraping practice that warrants ethical scrutiny is parallelization. In the example of getting historical `@WhiteHouse` tweets, we launched a single browser window and \"scrolled\" until we reached the end; a process that took on the order of a minute.\n",
    "\n",
    "However, we *could* launch multiple scripts that each creates a browser windows and collect different segments of the data in parallel for us to combine the results at the end. In an API context, we *could* create multiple applications and design our requests so that each works simultaneously to get all the data. \n",
    "\n",
    "Each request imposes some cost on the server to receive, process, and return the requested data: making these requests in parallel increases the convenience and efficiency for the data scraper, but also dramatically increases the strain on the server to fulfill other clients' requests. In fact, highly-parallelized and synchronized requests can look like [denial-of-service attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack) and may get your requests far more scrutiny and blowback than patiently waiting for your data to arrive in series. The ethical justifications for employing highly-parallelized scraping approaches are thin: documenting a rapidly-unfolding event before the data disappears, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Internet Archive's Wayback Machine\n",
    "\n",
    "Now we'll leave some of the ethically-fraught methods of web scraping behind. The Internet Archive maintains the \"[Wayback Machine](https://www.archive.org/web/)\" where old versions of websites are stored. Some of my favorites:\n",
    "\n",
    "* [CNN in June 2000](https://web.archive.org/web/20000815052826/http://www.cnn.com/)\n",
    "* [Facebook in August 2004](https://web.archive.org/web/20040817020419/http://www.facebook.com/)\n",
    "* [Apple in April 1997](https://web.archive.org/web/19970404064444/http://www.apple.com:80/)\n",
    "\n",
    "In these URLs above, there is a numeric identifier corresponding to the timestamp when the image of the website was captured. How do we know when the Wayback Machine archived a webpage? There's a free and open API!\n",
    "\n",
    "### Using the Wayback Machine API\n",
    "\n",
    "The simplest API request we can make asks for the most recent snapshot of a webpage archived by the Wayback Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_url = 'http://archive.org/wayback/available?url=facebook.com'\n",
    "\n",
    "wb_response = requests.get(wb_url)\n",
    "\n",
    "wb_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This response tells us the timestamp and location of this snapshot, which we could then go retrieve and parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_response_json = wb_response.json()\n",
    "\n",
    "recent_fb_wb_url = wb_response_json['archived_snapshots']['closest']['url']\n",
    "\n",
    "recent_fb_wb_response = requests.get(recent_fb_wb_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the raw text out, soupify, and look for links. For some reason all the links in this snapshot are in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_fb_wb_raw = recent_fb_wb_response.text\n",
    "\n",
    "recent_fb_wb_soup = BeautifulSoup(recent_fb_wb_raw)\n",
    "\n",
    "[link.text for link in recent_fb_wb_soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask for the most recent snapshot of a webpage around a specific date. Let's ask the Wayback Machine for a snapshot of Facebook around February 1, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_url = 'http://archive.org/wayback/available?url=facebook.com&timestamp=20080201'\n",
    "\n",
    "wb_response = requests.get(wb_url)\n",
    "\n",
    "wb_response_json = wb_response.json()\n",
    "\n",
    "wb_response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a relatively deep JSON object we have to navigate into to access information like the Wayback URL or the timestamp of the snapshot. The closest snapshot to February 1, 2008 was January 30, 2008. We use the [`datetime.strptime`](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) function to turn this numeric string that we recognize as a timestamp into a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.strptime(wb_response_json['archived_snapshots']['closest']['timestamp'],\n",
    "                        '%Y%m%d%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we could scrape out the links on this 2008 version of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the old URL\n",
    "fb_wb_url = wb_response_json['archived_snapshots']['closest']['url']\n",
    "\n",
    "# Go get the archived snapshot from the Wayback Machine\n",
    "fb_wb_response = requests.get(fb_wb_url)\n",
    "\n",
    "# Get the text from the response\n",
    "fb_wb_raw = fb_wb_response.text\n",
    "\n",
    "# Soup-ify\n",
    "fb_wb_soup = BeautifulSoup(fb_wb_raw)\n",
    "\n",
    "# Make a list of the text of the links\n",
    "[link.text for link in fb_wb_soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could likewise launch this link to view the page in Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome(executable_path='/Users/briankeegan/Dropbox/Courses/2019 Spring - ITSS Web Data Scraping/chromedriver')\n",
    "\n",
    "driver.get(fb_wb_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping historical web pages\n",
    "\n",
    "A current project I am working on is exploring how social media platforms' terms of service have evolved over time. Let's start with Facebook's terms of service and privacy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_tos = 'http://www.facebook.com/terms.php'\n",
    "fb_pp = 'http://www.facebook.com/policy.php'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take advantage of the [`date_range`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html) fuction in `pandas` to generate a range of dates between January 2005 and January 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_list = pd.date_range(start='2005-01-01',end='2019-01-01',freq='M')\n",
    "dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [`datetime.strftime`](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) (the inverse of `strptime`) to make these date objects into specifically-formatted strings that we can format into a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first datetime object and turn it into a string\n",
    "datetime.strftime(dates_list[0],'%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use string formatting to put the `fb_tos` URL and formatted timestamp into a request to the Wayback Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.strftime(dates_list[0],'%Y%m%d')\n",
    "\n",
    "wb_api_url = 'https://archive.org/wayback/available?url={0}&timestamp={1}'\n",
    "wb_api_url_formatted = wb_api_url.format(fb_tos,date_str)\n",
    "\n",
    "print(wb_api_url_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the request to the Wayback Machine to get the URL and timestamp of the Wayback Machine's closest snapshot of Facebook's Terms of Service before January 31, 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_api_response = requests.get(wb_api_url_formatted)\n",
    "\n",
    "wb_api_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the markup of this old version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the old URL\n",
    "wb_fb_old_url = wb_api_response.json()['archived_snapshots']['closest']['url']\n",
    "\n",
    "# Go get the archived snapshot from the Wayback Machine\n",
    "wb_fb_raw = requests.get(wb_fb_old_url).text\n",
    "\n",
    "# Soup-ify\n",
    "wb_fb_soup = BeautifulSoup(wb_fb_raw)\n",
    "\n",
    "# Find the content element and get the text out\n",
    "wb_fb_terms_str = wb_fb_soup.find('div',{'id':'content'}).text.strip()\n",
    "\n",
    "# Inspect\n",
    "wb_fb_terms_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a really dumb stemmer, [`.split()`](https://docs.python.org/3.7/library/stdtypes.html#str.split) to count the number of words in these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wb_fb_terms_str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop to find a snapshot of Facebook's ToS each month in our `dates_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(url_str,start_date='2005-01-01',end_date='2019-01-01',freq='M'):\n",
    "    \n",
    "    # Make the list of dates\n",
    "    date_l = pd.date_range(start_date,end_date,freq=freq)\n",
    "    \n",
    "    # Create an empty container to store our data\n",
    "    urls = dict()\n",
    "\n",
    "    # For each date in the list of dates\n",
    "    for date in date_l:\n",
    "        \n",
    "        # Turn the date object back into a string\n",
    "        date_str = datetime.strftime(date,'%Y%m%d%H%M%S')\n",
    "        \n",
    "        # Define the API URL request to the Wayback machine\n",
    "        wb_api_url = 'http://archive.org/wayback/available?url={0}&timestamp={1}'\n",
    "        \n",
    "        # Format the API URL with the URL of the website and the closest datetime\n",
    "        wb_api_request = wb_api_url.format(url_str,date_str)\n",
    "        \n",
    "        # Make the request\n",
    "        r = requests.get(wb_api_request).json()\n",
    "\n",
    "        # Check if the returned request has all the right parts (this is probably overkill)\n",
    "        if 'archived_snapshots' in r.keys():\n",
    "            if 'closest' in r['archived_snapshots'].keys():\n",
    "                if 'url' in r['archived_snapshots']['closest'].keys():\n",
    "                    \n",
    "                    # If it does have all the right parts, get the URL\n",
    "                    _url = r['archived_snapshots']['closest']['url']\n",
    "                    \n",
    "                    # Get the timestamp\n",
    "                    _timestamp = r['archived_snapshots']['closest']['timestamp']\n",
    "                    \n",
    "                    # Save to our URL dictionary with the timestamp of the snapshot as key, the url as value\n",
    "                    urls[_timestamp] = _url\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our function to make a dictionary of keys returning the Wayback Machine URLs for each month's version of the terms of service. We'll write a loop to get the Terms for each snapshot and count the words. \n",
    "\n",
    "This will take a few minutes. \n",
    "\n",
    "I've coverted the code block into a \"Raw\" cell to prevent accidental execution. You can always turn it into a \"Code\" cell if you really want to run it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get the list of timestamps and URLs for each monthly version of the Terms of Service\n",
    "fb_terms_d = get_urls('https://www.facebook.com/terms.php')\n",
    "\n",
    "# Create an empty container to store our data\n",
    "fb_terms_wordcount = {}\n",
    "\n",
    "# Loop through the fb_terms_d dictionary\n",
    "for timestamp,url in fb_terms_d.items():\n",
    "\n",
    "    # Get the raw HTML from the Wayback Machine\n",
    "    raw = requests.get(url).text\n",
    "    \n",
    "    # Soup-ify\n",
    "    soup = BeautifulSoup(raw)\n",
    "    \n",
    "    # Find the content of the TOS\n",
    "    content = soup.find('div',{'id':'content'}).text.strip()\n",
    "    \n",
    "    # Split the content into words, count the number of words, save to the container\n",
    "    fb_terms_wordcount[timestamp] = len(content.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having everyone hit the Internet Archive server with the same requests, you can also load this file with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('facebook_tos_archive.json','r') as f:\n",
    "    fb_terms_wordcount2 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the changes in the size of Facebook's Terms of Service over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dictionary into a pandas Series\n",
    "fb_terms_s = pd.Series(fb_terms_wordcount2)\n",
    "\n",
    "# Conver the index to datetime objects\n",
    "fb_terms_s.index = pd.to_datetime(fb_terms_s.index)\n",
    "\n",
    "# Plot\n",
    "ax = fb_terms_s.plot()\n",
    "\n",
    "# Make the x-tick labels less weird\n",
    "ax.set_xticklabels(range(2004,2019,2),rotation=0,horizontalalignment='center')\n",
    "\n",
    "# Always label your axes\n",
    "ax.set_ylabel('Word count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
